{"metadata":{"jupytext":{"formats":"md,ipynb"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**This notebook is an exercise in the [Computer Vision](https://www.kaggle.com/learn/computer-vision) course.  You can reference the tutorial at [this link](https://www.kaggle.com/ryanholbrook/maximum-pooling).**\n\n---\n","metadata":{}},{"cell_type":"markdown","source":"# Introduction #\n\nIn these exercises, you'll conclude the feature extraction begun in Exercise 2, explore how invariance is created by maximum pooling, and then look at a different kind of pooling: *average* pooling.\n\nRun the cell below to set everything up.","metadata":{}},{"cell_type":"code","source":"# Setup feedback system\nfrom learntools.core import binder\nbinder.bind(globals())\nfrom learntools.computer_vision.ex3 import *\n\nimport numpy as np\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom matplotlib import gridspec\nimport learntools.computer_vision.visiontools as visiontools\n\nplt.rc('figure', autolayout=True)\nplt.rc('axes', labelweight='bold', labelsize='large',\n       titleweight='bold', titlesize=18, titlepad=10)\nplt.rc('image', cmap='magma')","metadata":{"execution":{"iopub.status.busy":"2022-07-18T00:54:00.475878Z","iopub.execute_input":"2022-07-18T00:54:00.477237Z","iopub.status.idle":"2022-07-18T00:54:11.185272Z","shell.execute_reply.started":"2022-07-18T00:54:00.477127Z","shell.execute_reply":"2022-07-18T00:54:11.184068Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"Run this cell to get back to where you left off in the previous lesson. We'll use a predefined kernel this time.","metadata":{}},{"cell_type":"code","source":"# Read image\nimage_path = '../input/computer-vision-resources/car_illus.jpg'\nimage = tf.io.read_file(image_path)\nimage = tf.io.decode_jpeg(image, channels=1)\nimage = tf.image.resize(image, size=[400, 400])\n\n# Embossing kernel\nkernel = tf.constant([\n    [-2, -1, 0],\n    [-1, 1, 1],\n    [0, 1, 2],\n])\n\n# Reformat for batch compatibility.\nimage = tf.image.convert_image_dtype(image, dtype=tf.float32)\nimage = tf.expand_dims(image, axis=0)\nkernel = tf.reshape(kernel, [*kernel.shape, 1, 1])\nkernel = tf.cast(kernel, dtype=tf.float32)\n\nimage_filter = tf.nn.conv2d(\n    input=image,\n    filters=kernel,\n    strides=1,\n    padding='VALID',\n)\n\nimage_detect = tf.nn.relu(image_filter)\n\n# Show what we have so far\nplt.figure(figsize=(12, 6))\nplt.subplot(131)\nplt.imshow(tf.squeeze(image), cmap='gray')\nplt.axis('off')\nplt.title('Input')\nplt.subplot(132)\nplt.imshow(tf.squeeze(image_filter))\nplt.axis('off')\nplt.title('Filter')\nplt.subplot(133)\nplt.imshow(tf.squeeze(image_detect))\nplt.axis('off')\nplt.title('Detect')\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-07-18T00:54:25.606805Z","iopub.execute_input":"2022-07-18T00:54:25.607445Z","iopub.status.idle":"2022-07-18T00:54:26.536675Z","shell.execute_reply.started":"2022-07-18T00:54:25.607410Z","shell.execute_reply":"2022-07-18T00:54:26.535494Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"# 1) Apply Pooling to Condense #\n\nFor for the last step in the sequence, apply maximum pooling using a $2 \\times 2$ pooling window. You can copy this code to get started:\n\n```\nimage_condense = tf.nn.pool(\n    input=image_detect,\n    window_shape=____,\n    pooling_type=____,\n    strides=(2, 2),\n    padding='SAME',\n)\n```","metadata":{"lines_to_next_cell":0}},{"cell_type":"code","source":"# YOUR CODE HERE\nimage_condense = tf.nn.pool(\n    input=image_detect,\n    window_shape=(2, 2),\n    pooling_type='MAX',\n    strides=(2, 2),\n    padding='SAME',\n)\n\n# Check your answer\nq_1.check()","metadata":{"lines_to_next_cell":0,"execution":{"iopub.status.busy":"2022-07-18T00:54:53.300022Z","iopub.execute_input":"2022-07-18T00:54:53.300522Z","iopub.status.idle":"2022-07-18T00:54:53.325461Z","shell.execute_reply.started":"2022-07-18T00:54:53.300483Z","shell.execute_reply":"2022-07-18T00:54:53.323873Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"# Lines below will give you a hint or solution code\n#q_1.hint()\n#q_1.solution()","metadata":{"execution":{"iopub.status.busy":"2022-07-18T00:54:58.241652Z","iopub.execute_input":"2022-07-18T00:54:58.242351Z","iopub.status.idle":"2022-07-18T00:54:58.249986Z","shell.execute_reply.started":"2022-07-18T00:54:58.242294Z","shell.execute_reply":"2022-07-18T00:54:58.247804Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":"Run the next cell to see what maximum pooling did to the feature!","metadata":{}},{"cell_type":"code","source":"plt.figure(figsize=(8, 6))\nplt.subplot(121)\nplt.imshow(tf.squeeze(image_detect))\nplt.axis('off')\nplt.title(\"Detect (ReLU)\")\nplt.subplot(122)\nplt.imshow(tf.squeeze(image_condense))\nplt.axis('off')\nplt.title(\"Condense (MaxPool)\")\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-07-18T00:55:01.349827Z","iopub.execute_input":"2022-07-18T00:55:01.351039Z","iopub.status.idle":"2022-07-18T00:55:01.662757Z","shell.execute_reply.started":"2022-07-18T00:55:01.351001Z","shell.execute_reply":"2022-07-18T00:55:01.661665Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"We learned about how `MaxPool2D` layers give a convolutional network the property of **translation invariance** over small distances. In this exercise, you'll have a chance to observe this in action.\n\nThis next code cell will randomly apply a small shift to a circle and then condense the image several times with maximum pooling. Run the cell once and make note of the image that results at the end.","metadata":{}},{"cell_type":"code","source":"REPEATS = 4\nSIZE = [64, 64]\n\n# Create a randomly shifted circle\nimage = visiontools.circle(SIZE, r_shrink=4, val=1)\nimage = tf.expand_dims(image, axis=-1)\nimage = visiontools.random_transform(image, jitter=3, fill_method='replicate')\nimage = tf.squeeze(image)\n\nplt.figure(figsize=(16, 4))\nplt.subplot(1, REPEATS+1, 1)\nplt.imshow(image, vmin=0, vmax=1)\nplt.title(\"Original\\nShape: {}x{}\".format(image.shape[0], image.shape[1]))\nplt.axis('off')\n\n# Now condense with maximum pooling several times\nfor i in range(REPEATS):\n    ax = plt.subplot(1, REPEATS+1, i+2)\n    image = tf.reshape(image, [1, *image.shape, 1])\n    image = tf.nn.pool(image, window_shape=(2,2), strides=(2, 2), padding='SAME', pooling_type='MAX')\n    image = tf.squeeze(image)\n    plt.imshow(image, vmin=0, vmax=1)\n    plt.title(\"MaxPool {}\\nShape: {}x{}\".format(i+1, image.shape[0], image.shape[1]))\n    plt.axis('off')","metadata":{"execution":{"iopub.status.busy":"2022-07-18T00:55:16.321501Z","iopub.execute_input":"2022-07-18T00:55:16.322004Z","iopub.status.idle":"2022-07-18T00:55:18.209256Z","shell.execute_reply.started":"2022-07-18T00:55:16.321964Z","shell.execute_reply":"2022-07-18T00:55:18.208184Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"# 2) Explore Invariance #\n\n\nSuppose you had made a small shift in a different direction -- what effect would you expect that have on the resulting image? Try running the cell a few more times, if you like, to get a new random shift.","metadata":{}},{"cell_type":"code","source":"# View the solution (Run this code cell to receive credit!)\nq_2.solution()","metadata":{"execution":{"iopub.status.busy":"2022-07-18T00:55:40.469668Z","iopub.execute_input":"2022-07-18T00:55:40.470130Z","iopub.status.idle":"2022-07-18T00:55:40.481991Z","shell.execute_reply.started":"2022-07-18T00:55:40.470092Z","shell.execute_reply":"2022-07-18T00:55:40.480553Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# Global Average Pooling #\n\nWe mentioned in the previous exercise that average pooling has largely been superceeded by maximum pooling within the convolutional base. There is, however, a kind of average pooling that is still widely used in the *head* of a convnet. This is **global average pooling**. A `GlobalAvgPool2D` layer is often used as an alternative to some or all of the hidden `Dense` layers in the head of the network, like so:\n\n```\nmodel = keras.Sequential([\n    pretrained_base,\n    layers.GlobalAvgPool2D(),\n    layers.Dense(1, activation='sigmoid'),\n])\n```\n\nWhat is this layer doing? Notice that we no longer have the `Flatten` layer that usually comes after the base to transform the 2D feature data to 1D data needed by the classifier. Now the `GlobalAvgPool2D` layer is serving this function. But, instead of \"unstacking\" the feature (like `Flatten`), it simply replaces the entire feature map with its average value. Though very destructive, it often works quite well and has the advantage of reducing the number of parameters in the model.\n\nLet's look at what `GlobalAvgPool2D` does on some randomly generated feature maps. This will help us to understand how it can \"flatten\" the stack of feature maps produced by the base.\n\nRun this next cell a few times until you get a feel for how this new layer works.","metadata":{"lines_to_next_cell":0}},{"cell_type":"code","source":"feature_maps = [visiontools.random_map([5, 5], scale=0.1, decay_power=4) for _ in range(8)]\n\ngs = gridspec.GridSpec(1, 8, wspace=0.01, hspace=0.01)\nplt.figure(figsize=(18, 2))\nfor i, feature_map in enumerate(feature_maps):\n    plt.subplot(gs[i])\n    plt.imshow(feature_map, vmin=0, vmax=1)\n    plt.axis('off')\nplt.suptitle('Feature Maps', size=18, weight='bold', y=1.1)\nplt.show()\n\n# reformat for TensorFlow\nfeature_maps_tf = [tf.reshape(feature_map, [1, *feature_map.shape, 1])\n                   for feature_map in feature_maps]\n\nglobal_avg_pool = tf.keras.layers.GlobalAvgPool2D()\npooled_maps = [global_avg_pool(feature_map) for feature_map in feature_maps_tf]\nimg = np.array(pooled_maps)[:,:,0].T\n\nplt.imshow(img, vmin=0, vmax=1)\nplt.axis('off')\nplt.title('Pooled Feature Maps')\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-07-18T00:56:35.589896Z","iopub.execute_input":"2022-07-18T00:56:35.590341Z","iopub.status.idle":"2022-07-18T00:56:37.835377Z","shell.execute_reply.started":"2022-07-18T00:56:35.590305Z","shell.execute_reply":"2022-07-18T00:56:37.834364Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"Since each of the $5 \\times 5$ feature maps was reduced to a single value, global pooling reduced the number of parameters needed to represent these features by a factor of 25 -- a substantial savings!\n\nNow we'll move on to understanding the pooled features.\n\nAfter we've pooled the features into just a single value, does the head still have enough information to determine a class? This part of the exercise will investigate that question.\n\nLet's pass some images from our *Car or Truck* dataset through VGG16 and examine the features that result after pooling. First run this cell to define the model and load the dataset.","metadata":{}},{"cell_type":"code","source":"from tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\n\n# Load VGG16\npretrained_base = tf.keras.models.load_model(\n    '../input/cv-course-models/cv-course-models/vgg16-pretrained-base',\n)\n\nmodel = keras.Sequential([\n    pretrained_base,\n    # Attach a global average pooling layer after the base\n    layers.GlobalAvgPool2D(),\n])\n\n# Load dataset\nds = image_dataset_from_directory(\n    '../input/car-or-truck/train',\n    labels='inferred',\n    label_mode='binary',\n    image_size=[128, 128],\n    interpolation='nearest',\n    batch_size=1,\n    shuffle=True,\n)\n\nds_iter = iter(ds)","metadata":{"execution":{"iopub.status.busy":"2022-07-18T00:59:02.214630Z","iopub.execute_input":"2022-07-18T00:59:02.215222Z","iopub.status.idle":"2022-07-18T00:59:08.135593Z","shell.execute_reply.started":"2022-07-18T00:59:02.215159Z","shell.execute_reply":"2022-07-18T00:59:08.134400Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"Notice how we've attached a `GlobalAvgPool2D` layer after the pretrained VGG16 base. Ordinarily, VGG16 will produce 512 feature maps for each image. The `GlobalAvgPool2D` layer reduces each of these to a single value, an \"average pixel\", if you like.\n\nThis next cell will run an image from the *Car or Truck* dataset through VGG16 and show you the 512 average pixels created by `GlobalAvgPool2D`. Run the cell a few times and observe the pixels produced by cars versus the pixels produced by trucks.","metadata":{}},{"cell_type":"code","source":"car = next(ds_iter)\n\ncar_tf = tf.image.resize(car[0], size=[128, 128])\ncar_features = model(car_tf)\ncar_features = tf.reshape(car_features, shape=(16, 32))\nlabel = int(tf.squeeze(car[1]).numpy())\n\nplt.figure(figsize=(8, 4))\nplt.subplot(121)\nplt.imshow(tf.squeeze(car[0]))\nplt.axis('off')\nplt.title([\"Car\", \"Truck\"][label])\nplt.subplot(122)\nplt.imshow(car_features)\nplt.title('Pooled Feature Maps')\nplt.axis('off')\nplt.show();","metadata":{"execution":{"iopub.status.busy":"2022-07-18T00:59:18.173581Z","iopub.execute_input":"2022-07-18T00:59:18.174341Z","iopub.status.idle":"2022-07-18T00:59:18.685838Z","shell.execute_reply.started":"2022-07-18T00:59:18.174299Z","shell.execute_reply":"2022-07-18T00:59:18.684269Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# 3) Understand the Pooled Features #\n\nWhat do you see? Are the pooled features for cars and trucks different enough to tell them apart? How would you interpret these pooled values? How could this help the classification? After you've thought about it, run the next cell for an answer. (Or see a hint first!)","metadata":{}},{"cell_type":"code","source":"# View the solution (Run this code cell to receive credit!)\nq_3.check()","metadata":{"execution":{"iopub.status.busy":"2022-07-18T01:00:08.864474Z","iopub.execute_input":"2022-07-18T01:00:08.864973Z","iopub.status.idle":"2022-07-18T01:00:08.876139Z","shell.execute_reply.started":"2022-07-18T01:00:08.864932Z","shell.execute_reply":"2022-07-18T01:00:08.874849Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"# Line below will give you a hint\n#q_3.hint()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Global average pooling is often used in modern convnets. One big advantage is that it greatly reduces the number of parameters in a model, while still telling you if some feature was present in an image or not -- which for classification is usually all that matters. If you're creating a convolutional classifier it's worth trying out!\n\n# Conclusion #\n\nIn this lesson we explored the final operation in the feature extraction process: **condensing** with **maximum pooling**. Pooling is one of the essential features of convolutional networks and helps provide them with some of their characteristic advantages: efficiency with visual data, reduced parameter size compared to dense networks, translation invariance. We've seen that it's used not only in the base during feature extraction, but also can be used in the head during classification. Understanding it is essential to a full understanding of convnets.\n\n# Keep Going #\n\nIn the next lesson, we'll conclude our discussion of the feature extraction operations with **sliding windows**, the typical way of describing how the convolution and pooling operations scan over an image. We'll describe here the final two parameters in the `Conv2D` and `MaxPool2D` layers: `strides` and `padding`. [**Check it out**](https://www.kaggle.com/ryanholbrook/the-sliding-window) now!","metadata":{}},{"cell_type":"markdown","source":"---\n\n\n\n\n*Have questions or comments? Visit the [course discussion forum](https://www.kaggle.com/learn/computer-vision/discussion) to chat with other learners.*","metadata":{}}]}